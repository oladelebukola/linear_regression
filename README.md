# Welcome to My Linear Regression
## Task
The task was an exploration into solving logical and linear regressions models/problems

## Description
I used the following functions:
def h(x, theta):took two parameters: x and theta. It calculates the dot product between x and theta using np.dot()function and reshapes
def mean_squared_error(y_predicted, y_label): to calculate the Mean Square Error between predicted values and labels.
class LeastSquaresRegression(): calculated the Î¸ feature weights using the normal equation and makes predictions
def bias_column(X): to add the bias column to the feature matrix X
class LeastSquaresRegression():I used it to generate self.theta_ whic was able to check if the learned weights are consistent with the generating equation
def test_h_function_II(self): It checked whether the function h(x, theta) produced the expected output.
class GradientDescentOptimizer():I used it to find the best theta value and see the corresponding function value and gradient.

## Installation
There was no need for installation as it can easily be run on local machine using jupyter notebook and other necessary dependencies

## Usage
The code was run on a jupyter notebook whose http address can be gotten by running this in the terminal:
jupyter notebook --no-browser
Then the jupyter http address can be followed and the password: 'qwasar' inputed

### The Core Team
Bukola Veronica Oladele(veronica_b)

<span><i>Made at <a href='https://qwasar.io'>Qwasar SV -- Software Engineering School</a></i></span>
<span><img alt='Qwasar SV -- Software Engineering School's Logo' src='https://storage.googleapis.com/qwasar-public/qwasar-logo_50x50.png' width='20px'></span>
