{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d230ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce91edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, theta):\n",
    "    return np.matmul(x,theta.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "566dd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_predicted, y_label):\n",
    "    m = len(y_label)  # Number of instances\n",
    "    squared_errors = np.square(y_predicted - y_label)\n",
    "    mse = np.mean(squared_errors)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d43b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresRegression():\n",
    "    def __init__(self):\n",
    "        self.theta_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Add a column of ones to the feature matrix for the bias term\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Calculate theta using the normal equation\n",
    "        X_T = np.transpose(X)\n",
    "        X_T_X_inv = np.linalg.inv(np.dot(X_T, X))\n",
    "        self.theta_ = np.dot(np.dot(X_T_X_inv, X_T), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add a column of ones to the feature matrix for the bias term\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        # Make predictions using the learned theta\n",
    "        return np.dot(X, self.theta_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7ee25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data\n",
    "X = 4 * np.random.rand(100, 1)\n",
    "y = 10 + 2 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Data Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2a6ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.91340515]\n",
      " [1.82940457]\n",
      " [0.55930973]\n",
      " [2.63698448]\n",
      " [3.56208649]]\n",
      " ---- \n",
      "[[1.         0.91340515]\n",
      " [1.         1.82940457]\n",
      " [1.         0.55930973]\n",
      " [1.         2.63698448]\n",
      " [1.         3.56208649]]\n"
     ]
    }
   ],
   "source": [
    "def bias_column(X):\n",
    "    # Add a column of ones at the beginning of the feature matrix\n",
    "    ones_column = np.ones((X.shape[0], 1))\n",
    "    X_new = np.hstack((ones_column, X))\n",
    "    return X_new\n",
    "\n",
    "X = np.array([[0.91340515], [1.82940457], [0.55930973], [2.63698448], [3.56208649]])\n",
    "\n",
    "X_new = bias_column(X)\n",
    "\n",
    "print(X[:5])\n",
    "print(\" ---- \")\n",
    "print(X_new[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb34134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresRegression():\n",
    "    def __init__(self):\n",
    "        self.theta_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate theta using the normal equation\n",
    "        X_T = np.transpose(X)\n",
    "        X_T_X_inv = np.linalg.inv(np.dot(X_T, X))\n",
    "        self.theta_ = np.dot(np.dot(X_T_X_inv, X_T), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the learned theta\n",
    "        return np.dot(X, self.theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d236dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new shape: (10, 1)\n",
      "y shape: (10, 1)\n",
      "Learned Weights:\n",
      "[[6.4454785]]\n",
      "Expected Weights:\n",
      "[[10]\n",
      " [ 2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_new = X_new.reshape(-1, 1)  # Reshape X_new to have shape (n_samples, 1)\n",
    "y = y[:X_new.shape[0]].reshape(-1, 1)  # Reshape y to match the number of samples in X_new\n",
    "print(\"X_new shape:\", X_new.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "model = LeastSquaresRegression()\n",
    "model.fit(X_new, y)\n",
    "\n",
    "print(\"Learned Weights:\")\n",
    "print(model.theta_)\n",
    "\n",
    "expected_weights = np.array([[10], [2]])  # Expected weights from the generating equation\n",
    "print(\"Expected Weights:\")\n",
    "print(expected_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b354f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_h_function_II(self):\n",
    "    x = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "    theta = np.array([[1], [2]])  # Reshape theta to have shape (2, 1)\n",
    "    y_predicted = eg.h(x, theta)\n",
    "\n",
    "    expected_output = np.array([[3], [5], [7]])\n",
    "    self.assertTrue((y_predicted == expected_output).all())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fbb46cc",
   "metadata": {},
   "source": [
    "By comparing the learned weights (model.theta_) with the expected weights (expected_weights), you can see if they are consistent or close to the generating equation. If the learned weights are close to the expected values of 10 and 2, then it indicates that the model has learned the underlying linear relationship in the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "84f45c3b",
   "metadata": {},
   "source": [
    "Comparison:\n",
    "The computational complexity of the least squares regression method depends on the number of features (n) and the number of instances or samples (m).In terms of complexity, the training complexity of least squares regression dominates the prediction complexity. The training complexity is higher due to the need to invert the feature matrix, while the prediction complexity is relatively lower and scales linearly with the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f61502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # Define the function f(x)\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    result = 3 + np.dot(diff.T, diff)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e29556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    result = 3 + np.dot(diff.T, diff)\n",
    "    return result\n",
    "\n",
    "def fprime(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    gradient = 2 * diff\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c069309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta found is [[1.93133971]\n",
      " [5.35962424]]\n",
      "Value of f at this theta: f(theta) = [[3.41479535]]\n",
      "Value of f prime at this theta: f'(theta) = [[-0.13732058]\n",
      " [-1.28075152]]\n"
     ]
    }
   ],
   "source": [
    "# Define the function f(x) and its gradient fprime(x)\n",
    "def f(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    result = 3 + np.dot(diff.T, diff)\n",
    "    return result\n",
    "\n",
    "def fprime(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    gradient = 2 * diff\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# Define the GradientDescentOptimizer class\n",
    "class GradientDescentOptimizer():\n",
    "    def __init__(self, f, fprime, start, learning_rate=0.1):\n",
    "        self.f_ = f                       # The function\n",
    "        self.fprime_ = fprime              # The gradient of f\n",
    "        self.current_ = start.reshape(-1,1)              # The current point being evaluated\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.history_ = [start]            # Save history as attributes\n",
    "        self.gradient_history_ = []        # Save gradient history\n",
    "\n",
    "    def step(self):\n",
    "        # Take a gradient descent step\n",
    "        gradient = self.fprime_(self.current_)\n",
    "        self.current_ = self.current_ - self.learning_rate_ * gradient\n",
    "        self.history_.append(self.current_)\n",
    "        self.gradient_history_.append(gradient)\n",
    "\n",
    "    def optimize(self, iterations=100):\n",
    "        # Use gradient descent to get closer to the minimum\n",
    "        for _ in range(iterations):\n",
    "            self.step()\n",
    "\n",
    "    def getCurrentValue(self):\n",
    "        # Getter for current_\n",
    "        return self.current_\n",
    "\n",
    "    def print_result(self):\n",
    "        print(\"Best theta found is \" + str(self.current_))\n",
    "        print(\"Value of f at this theta: f(theta) = \" + str(self.f_(self.current_)))\n",
    "        print(\"Value of f prime at this theta: f'(theta) = \" + str(self.fprime_(self.current_)))\n",
    "\n",
    "\n",
    "# Create an instance of the GradientDescentOptimizer class and optimize\n",
    "grad = GradientDescentOptimizer(f, fprime, np.random.normal(size=(2,)), 0.1)\n",
    "grad.optimize(10)\n",
    "grad.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function f(x)\n",
    "def f(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    result = 3 + np.dot(diff.T, diff)\n",
    "    return result\n",
    "\n",
    "# Generate data for plotting\n",
    "x1 = np.linspace(-10, 10, 100)\n",
    "x2 = np.linspace(-10, 10, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Z = f(np.vstack((X1.flatten(), X2.flatten()))).reshape(X1.shape)\n",
    "# Reshape theta to (2, 1) to match the number of columns in x\n",
    "theta = theta.reshape(-1, 1)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X1, X2, Z, cmap='viridis')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12098854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set labels and title\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('f(x)')\n",
    "ax.set_title('Function f(x)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be016cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function f(x) and its gradient fprime(x)\n",
    "def f(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    result = 3 + np.dot(diff.T, diff)\n",
    "    return result\n",
    "\n",
    "def fprime(x):\n",
    "    target = np.array([[2], [6]])  # Target vector (2 6)T\n",
    "    diff = x - target\n",
    "    gradient = 2 * diff\n",
    "    return gradient\n",
    "\n",
    "# Define the GradientDescentOptimizer class\n",
    "class GradientDescentOptimizer():\n",
    "    def __init__(self, f, fprime, start, learning_rate=0.1):\n",
    "        self.f_ = f                       # The function\n",
    "        self.fprime_ = fprime              # The gradient of f\n",
    "        self.current_ = start.reshape(-1,1)              # The current point being evaluated\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.history_ = [start]            # Save history as attributes\n",
    "        self.gradient_history_ = []        # Save gradient history\n",
    "\n",
    "    def step(self):\n",
    "        # Take a gradient descent step\n",
    "        gradient = self.fprime_(self.current_)\n",
    "        self.current_ = self.current_ - self.learning_rate_ * gradient\n",
    "        self.history_.append(self.current_)\n",
    "        self.gradient_history_.append(gradient)\n",
    "\n",
    "    def optimize(self, iterations=100):\n",
    "        # Use gradient descent to get closer to the minimum\n",
    "        for _ in range(iterations):\n",
    "            self.step()\n",
    "\n",
    "    def getCurrentValue(self):\n",
    "        # Getter for current_\n",
    "        return self.current_\n",
    "\n",
    "    def print_result(self):\n",
    "        print(\"Best theta found is \" + str(self.current_))\n",
    "        print(\"Value of f at this theta: f(theta) = \" + str(self.f_(self.current_)))\n",
    "        print(\"Value of f prime at this theta: f'(theta) = \" + str(self.fprime_(self.current_)))\n",
    "\n",
    "\n",
    "# Create an instance of the GradientDescentOptimizer class and optimize\n",
    "grad = GradientDescentOptimizer(f, fprime, np.random.normal(size=(2,)), 0.1)\n",
    "grad.optimize(10)\n",
    "grad.print_result()\n",
    "\n",
    "# Plot the progression of the gradient\n",
    "gradient_history = grad.gradient_history_\n",
    "num_iterations = len(gradient_history)\n",
    "iteration_indices = range(num_iterations)\n",
    "x = [grad.history_[i][0] for i in iteration_indices]\n",
    "y = [grad.history_[i][1] for i in iteration_indices]\n",
    "z = [gradient_history[i][1] for i in iteration_indices]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(x, y, z, marker='o', linestyle='-', color='b')\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('Gradient')\n",
    "ax.set_title('Progression of Gradient')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5018d7d",
   "metadata": {},
   "source": [
    "The learning rate determines the step size taken in each iteration of the gradient descent algorithm. It controls how quickly or slowly the algorithm converges towards the minimum of the function. a learning rate that is too small may result in slow convergence, while a learning rate that is too large may cause instability.\n",
    "The number of iterations determines how many steps the optimization algorithm takes to converge or reach a stopping criterion. The optimization process may terminate prematurely, and the solution may not be optimal."
   ]
  },
  {
   "cell_type": "raw",
   "id": "87a0547a",
   "metadata": {},
   "source": [
    "Turning Hyperparameters and choosing a good learning rate includes;\n",
    "1) Grid Search or Random Search\n",
    "2) Learning Rate Schedule\n",
    "3) Learning Rate Visualization\n",
    "4) Learning Rate Decay"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ee51eac",
   "metadata": {},
   "source": [
    "The number of iterations is another important hyperparameter that can affect the optimization process. It determines how many steps the optimization algorithm takes to converge or reach a stopping criterion. The convergence rate refers to the speed at which the algorithm converges to the optimal solution.\n",
    "The convergence rate can be influenced by several factors including Complexity of the problem, Learning rate, and Gradient landscape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
